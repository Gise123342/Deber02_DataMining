{"block_file": {"custom/dbt_env_setup.py:custom:python:dbt env setup": {"content": "import os\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef dbt_env_setup(*args, **kwargs):\n    \"\"\"\n    Restaura el comportamiento anterior:\n    Carga variables desde Mage Secrets sin forzar el esquema en el entorno.\n    Los modelos definen su propio schema en dbt (ej. schema='SILVER').\n    \"\"\"\n    env_vars = {\n        \"SNOWFLAKE_USER\": get_secret_value(\"SNOWFLAKE_USER\"),\n        \"SNOWFLAKE_PASSWORD\": get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        \"SNOWFLAKE_ACCOUNT\": get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        \"SNOWFLAKE_DEFAULT_WH\": get_secret_value(\"SNOWFLAKE_DEFAULT_WH\"),\n        \"SNOWFLAKE_DEFAULT_DB\": get_secret_value(\"SNOWFLAKE_DEFAULT_DB\"),\n        \"SNOWFLAKE_DEFAULT_SCHEMA\": get_secret_value(\"SNOWFLAKE_DEFAULT_SCHEMA\"),\n        \"SNOWFLAKE_ROLE\": get_secret_value(\"SNOWFLAKE_ROLE\"),\n    }\n\n    for key, value in env_vars.items():\n        if value is not None:\n            os.environ[key] = str(value)\n\n    os.environ[\"DBT_PROFILES_DIR\"] = \"/home/src/scheduler/dbt\"\n    os.environ[\"DBT_PROFILE_NAME\"] = \"interpolated\"\n\n    print(\"Variables de entorno configuradas para DBT (modo antiguo).\")\n    print(f\"DBT_PROFILES_DIR = {os.environ['DBT_PROFILES_DIR']}\")\n    print(f\"DBT_PROFILE_NAME = {os.environ['DBT_PROFILE_NAME']}\")\n    print(\"Variables cargadas:\", \", \".join(env_vars.keys()))\n\n\n@test\ndef test_output(*args, **kwargs):\n    required_vars = [\n        \"SNOWFLAKE_USER\",\n        \"SNOWFLAKE_PASSWORD\",\n        \"SNOWFLAKE_ACCOUNT\",\n        \"SNOWFLAKE_DEFAULT_WH\",\n        \"SNOWFLAKE_DEFAULT_DB\",\n        \"SNOWFLAKE_DEFAULT_SCHEMA\",\n        \"SNOWFLAKE_ROLE\",\n        \"DBT_PROFILES_DIR\",\n        \"DBT_PROFILE_NAME\",\n    ]\n\n    missing = [v for v in required_vars if v not in os.environ]\n    assert not missing, f\"Faltan variables en entorno: {missing}\"\n    print(\"Todas las variables necesarias est\u00e1n configuradas correctamente.\")\n", "file_path": "custom/dbt_env_setup.py", "language": "python", "type": "custom", "uuid": "dbt_env_setup"}, "custom/dbt_env_setup_gold.py:custom:python:dbt env setup gold": {"content": "import os\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef dbt_env_setup(*args, **kwargs):\n    \"\"\"\n    Carga variables de entorno para DBT desde los Secrets de Mage\n    y configura el schema din\u00e1mico GOLD.\n    \"\"\"\n\n    # Variables desde Mage Secrets\n    env_vars = {\n        \"SNOWFLAKE_USER\": get_secret_value(\"SNOWFLAKE_USER\"),\n        \"SNOWFLAKE_PASSWORD\": get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        \"SNOWFLAKE_ACCOUNT\": get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        \"SNOWFLAKE_DEFAULT_WH\": get_secret_value(\"SNOWFLAKE_DEFAULT_WH\"),\n        \"SNOWFLAKE_DEFAULT_DB\": get_secret_value(\"SNOWFLAKE_DEFAULT_DB\"),\n        \"SNOWFLAKE_DEFAULT_SCHEMA\": get_secret_value(\"SNOWFLAKE_DEFAULT_SCHEMA\"),\n        \"SNOWFLAKE_ROLE\": get_secret_value(\"SNOWFLAKE_ROLE\"),\n    }\n\n    # Cargar al entorno\n    for key, value in env_vars.items():\n        if value is not None:\n            os.environ[key] = str(value)\n\n    #  Forzar variables de entorno para DBT\n    os.environ[\"DBT_PROFILES_DIR\"] = \"/home/src/scheduler/dbt\"\n    os.environ[\"DBT_PROFILE_NAME\"] = \"interpolated\"\n\n    # Definir el schema GOLD solo para esta tuber\u00eda\n    os.environ[\"DBT_DEFAULT_SCHEMA\"] = \"GOLD\"\n\n    print(\"Variables de entorno configuradas para DBT (capa GOLD).\")\n    print(f\"DBT_PROFILES_DIR = {os.environ['DBT_PROFILES_DIR']}\")\n    print(f\"DBT_PROFILE_NAME = {os.environ['DBT_PROFILE_NAME']}\")\n    print(f\" DBT_DEFAULT_SCHEMA = {os.environ['DBT_DEFAULT_SCHEMA']}\")\n    print(\"Variables cargadas:\", \", \".join(env_vars.keys()))\n\n\n@test\ndef test_output(*args, **kwargs):\n    \"\"\"\n    Valida que las variables est\u00e9n correctamente configuradas\n    antes de ejecutar modelos DBT.\n    \"\"\"\n    required_vars = [\n        \"SNOWFLAKE_USER\",\n        \"SNOWFLAKE_PASSWORD\",\n        \"SNOWFLAKE_ACCOUNT\",\n        \"SNOWFLAKE_DEFAULT_WH\",\n        \"SNOWFLAKE_DEFAULT_DB\",\n        \"SNOWFLAKE_DEFAULT_SCHEMA\",\n        \"SNOWFLAKE_ROLE\",\n        \"DBT_PROFILES_DIR\",\n        \"DBT_PROFILE_NAME\",\n        \"DBT_DEFAULT_SCHEMA\",  \n    ]\n\n    missing = [v for v in required_vars if v not in os.environ]\n    assert not missing, f\"Faltan variables en entorno: {missing}\"\n    print(\"Todas las variables necesarias est\u00e1n configuradas correctamente para GOLD.\")\n", "file_path": "custom/dbt_env_setup_gold.py", "language": "python", "type": "custom", "uuid": "dbt_env_setup_gold"}, "custom/gold_test.py:custom:python:gold test": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport os\nimport subprocess\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\n\n@custom\ndef run_dbt_tests(*args, **kwargs):\n    \"\"\"\n    Ejecuta todos los tests de DBT definidos en el schema.yml\n    para la capa GOLD (dimensiones + fact_trips).\n    \"\"\"\n\n    # 1. Cargar variables de entorno desde Mage Secrets\n    env_vars = {\n        \"SNOWFLAKE_USER\": get_secret_value(\"SNOWFLAKE_USER\"),\n        \"SNOWFLAKE_PASSWORD\": get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        \"SNOWFLAKE_ACCOUNT\": get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        \"SNOWFLAKE_DEFAULT_WH\": get_secret_value(\"SNOWFLAKE_DEFAULT_WH\"),\n        \"SNOWFLAKE_DEFAULT_DB\": get_secret_value(\"SNOWFLAKE_DEFAULT_DB\"),\n        \"SNOWFLAKE_DEFAULT_SCHEMA\": get_secret_value(\"SNOWFLAKE_DEFAULT_SCHEMA\"),\n        \"SNOWFLAKE_ROLE\": get_secret_value(\"SNOWFLAKE_ROLE\"),\n    }\n\n    for key, value in env_vars.items():\n        if value is not None:\n            os.environ[key] = str(value)\n\n    # 2. Configuraci\u00f3n DBT para capa GOLD\n    os.environ[\"DBT_PROFILES_DIR\"] = \"/home/src/scheduler/dbt\"\n    os.environ[\"DBT_PROFILE_NAME\"] = \"interpolated\"\n    os.environ[\"DBT_DEFAULT_SCHEMA\"] = \"GOLD\"\n\n    print(\"Variables de entorno configuradas para DBT (capa GOLD).\")\n    print(f\"DBT_PROFILES_DIR = {os.environ['DBT_PROFILES_DIR']}\")\n    print(f\"DBT_PROFILE_NAME = {os.environ['DBT_PROFILE_NAME']}\")\n    print(f\"DBT_DEFAULT_SCHEMA = {os.environ['DBT_DEFAULT_SCHEMA']}\")\n    print(\"Variables cargadas:\", \", \".join(env_vars.keys()))\n\n    # 3. Ejecutar los tests de DBT usando subprocess\n    print(\"Ejecutando tests de la capa GOLD...\")\n\n    cmd = [\n        \"dbt\",\n        \"test\",\n        \"--project-dir\", \"/home/src/scheduler/dbt\",\n        \"--select\", \"gold\",\n        \"--profiles-dir\", \"/home/src/scheduler/dbt\"\n    ]\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    print(\"Resultado de los tests:\")\n    print(result.stdout)\n    if result.stderr:\n        print(\"Errores detectados:\")\n        print(result.stderr)\n\n    return result.stdout\n\n\n@test\ndef test_output(*args, **kwargs):\n    \"\"\"\n    Verifica que los tests de DBT se ejecutaron correctamente.\n    \"\"\"\n    print(\"Validaci\u00f3n: Bloque 'run_dbt_tests' ejecutado correctamente.\")\n", "file_path": "custom/gold_test.py", "language": "python", "type": "custom", "uuid": "gold_test"}, "custom/silver_tests.py:custom:python:silver tests": {"content": "import subprocess\nimport os\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n@custom\ndef run_dbt_tests(*args, **kwargs):\n    project_dir = \"/home/src/scheduler/dbt\"\n    command = [\n        \"dbt\", \"test\",\n        \"--project-dir\", project_dir,\n        \"--profiles-dir\", project_dir,\n        \"--select\", \"silver.*\"\n    ]\n\n    print(f\"\ud83d\ude80 Ejecutando DBT tests en {project_dir}...\")\n    result = subprocess.run(command, capture_output=True, text=True)\n\n    print(\"\ud83d\udcc4 STDOUT:\\n\", result.stdout)\n    print(\"\u26a0\ufe0f STDERR:\\n\", result.stderr)\n\n    if result.returncode == 0:\n        print(\"\u2705 Tests completados correctamente.\")\n    else:\n        raise Exception(\"\u274c Error en los tests DBT.\")\n", "file_path": "custom/silver_tests.py", "language": "python", "type": "custom", "uuid": "silver_tests"}, "data_exporters/eli.py:data_exporter:python:eli": {"content": "from sqlalchemy import create_engine, text\n\n    \nuser = \"USERSNOW\"\npassword = \"Password123456\"\naccount = \"alnwmmv-nj56428.sa-east-1.aws\"\nwarehouse = \"COMPUTE_WH\"\ndatabase = \"TU_DATABASE\"\nschema = \"RAW\"\nrole = \"ACCOUNTADMIN\"\n\nengine = create_engine(\n    f\"snowflake://{user}:{password}@alnwmmv-nj56428.sa-east-1.aws/{database}/{schema}?warehouse={warehouse}&role={role}\"\n)\n\nwith engine.connect() as conn:\n    result = conn.execute(text(\"SELECT CURRENT_VERSION()\")).fetchone()\n    print(\"\u2705 Conectado a Snowflake. Versi\u00f3n:\", result[0])\n", "file_path": "data_exporters/eli.py", "language": "python", "type": "data_exporter", "uuid": "eli"}, "data_exporters/export_raw.py:data_exporter:python:export raw": {"content": "import pandas as pd\nimport uuid\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_exporter\ndef export_data(df: pd.DataFrame, *args, **kwargs):\n    \"\"\"\n    Exporta datos crudos de m\u00faltiples servicios (Yellow y Green)\n    a Snowflake, creando una tabla RAW separada por servicio.\n    Aplica idempotencia: elimina previamente registros del mismo a\u00f1o/mes.\n    \"\"\"\n\n    # Credenciales desde Mage Secrets\n    user = get_secret_value(\"SNOWFLAKE_USER\")\n    password = get_secret_value(\"SNOWFLAKE_PASSWORD\")\n    account = get_secret_value(\"SNOWFLAKE_ACCOUNT\")\n    warehouse = get_secret_value(\"SNOWFLAKE_DEFAULT_WH\")\n    database = get_secret_value(\"SNOWFLAKE_DEFAULT_DB\")\n    schema = get_secret_value(\"SNOWFLAKE_DEFAULT_SCHEMA\")\n    role = get_secret_value(\"SNOWFLAKE_ROLE\")\n\n    # Conexi\u00f3n \u00fanica\n    conn = snowflake.connector.connect(\n        user=user,\n        password=password,\n        account=account,\n        warehouse=warehouse,\n        database=database,\n        schema=schema,\n        role=role,\n        insecure_mode=True\n    )\n    cursor = conn.cursor()\n\n    # Detectar servicios \u00fanicos en el dataframe\n    services = df[\"__service_type\"].unique()\n    print(f\"Detectados servicios: {', '.join(services)}\")\n\n    for service in services:\n        df_service = df[df[\"__service_type\"] == service].copy()\n        table_name = f\"{service}_trips\".upper()\n\n        run_id = str(df_service[\"__run_id\"].iloc[0])\n        year = int(df_service[\"__year\"].iloc[0])\n        month = int(df_service[\"__month\"].iloc[0])\n        row_count = len(df_service)\n\n        print(f\"Exportando {row_count:,} filas \u2192 {schema}.{table_name} ({service.upper()} {year}-{month:02d})\")\n\n        # Normalizar columnas datetime\n        datetime_cols = [c for c in df_service.columns if \"datetime\" in c.lower()]\n        for col in datetime_cols:\n            df_service[col] = pd.to_datetime(df_service[col], errors=\"coerce\")\n            df_service[col] = df_service[col].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        # Crear tabla si no existe\n        cols_def = []\n        for col, dtype in df_service.dtypes.items():\n            if col in datetime_cols:\n                col_type = \"TIMESTAMP_NTZ\"\n            elif \"int\" in str(dtype):\n                col_type = \"NUMBER\"\n            elif \"float\" in str(dtype):\n                col_type = \"FLOAT\"\n            else:\n                col_type = \"VARCHAR\"\n            cols_def.append(f'\"{col.upper()}\" {col_type}')\n\n        create_sql = f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema}.{table_name} (\n                {\", \".join(cols_def)}\n            );\n        \"\"\"\n        cursor.execute(create_sql)\n        print(f\"Tabla {schema}.{table_name} creada/verificada\")\n\n        # \ud83e\uddf9 IDEMPOTENCIA \u2192 borrar registros del mismo a\u00f1o/mes antes de insertar\n        delete_sql = f\"\"\"\n            DELETE FROM {schema}.{table_name}\n            WHERE \"__YEAR\" = {year} AND \"__MONTH\" = {month};\n        \"\"\"\n        cursor.execute(delete_sql)\n        conn.commit()\n        print(f\"Registros previos eliminados para {year}-{month:02d} ({service.upper()})\")\n\n        # Exportaci\u00f3n bulk\n        success, nchunks, nrows, _ = write_pandas(\n            conn,\n            df_service,\n            table_name,\n            schema=schema,\n            quote_identifiers=False,\n            auto_create_table=False\n        )\n        print(f\"Exportados {nrows:,} registros a {table_name} en {nchunks} chunks\")\n\n        # Auditor\u00eda\n        cursor.execute(f\"\"\"\n            CREATE TABLE IF NOT EXISTS {schema}.INGEST_AUDIT (\n                run_id VARCHAR,\n                service_type VARCHAR,\n                year NUMBER,\n                month NUMBER,\n                row_count NUMBER,\n                ingest_ts TIMESTAMP_NTZ\n            );\n        \"\"\")\n\n        cursor.execute(f\"\"\"\n            INSERT INTO {schema}.INGEST_AUDIT\n            (run_id, service_type, year, month, row_count, ingest_ts)\n            VALUES ('{run_id}', '{service}', {year}, {month}, {row_count},\n                    CONVERT_TIMEZONE('UTC', 'America/Guayaquil', CURRENT_TIMESTAMP));\n        \"\"\")\n        conn.commit()\n        print(f\"Auditor\u00eda insertada para {service.upper()} {year}-{month:02d}\")\n\n    conn.close()\n    print(\"Exportaci\u00f3n completada para todos los servicios.\")\n\n\n@test\ndef test_output(*args, **kwargs):\n    assert True, \"Exporter ejecutado correctamente\"\n", "file_path": "data_exporters/export_raw.py", "language": "python", "type": "data_exporter", "uuid": "export_raw"}, "data_exporters/export_taxi_zone.py:data_exporter:python:export taxi zone": {"content": "import os\nimport pandas as pd\nimport snowflake.connector\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport math\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_snowflake(*args, **kwargs):\n    \"\"\"\n    Descarga el CSV Taxi Zone Lookup y lo carga en Snowflake -> RAW.TAXI_ZONE_LOOKUP\n    Sin usar write_pandas (evita OCSP y S3)\n    \"\"\"\n\n    # 1\ufe0f\u20e3 Descargar CSV\n    url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n    df = pd.read_csv(url)\n    df.columns = [c.strip().upper() for c in df.columns]\n\n    print(f\"\ud83d\udce5 CSV cargado con {len(df)} filas y columnas: {list(df.columns)}\")\n\n    # 2\ufe0f\u20e3 Limpiar NaN \u2192 None (para SQL NULL)\n    df = df.where(pd.notnull(df), None)\n\n    # 3\ufe0f\u20e3 Conectar a Snowflake\n    conn = snowflake.connector.connect(\n        user=get_secret_value(\"SNOWFLAKE_USER\"),\n        password=get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        account=get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=get_secret_value(\"SNOWFLAKE_DEFAULT_WH\"),\n        database=get_secret_value(\"SNOWFLAKE_DEFAULT_DB\"),\n        schema=\"RAW\",\n        role=get_secret_value(\"SNOWFLAKE_ROLE\"),\n        ocsp_fail_open=True,\n        insecure_mode=True\n    )\n    cursor = conn.cursor()\n\n    # 4\ufe0f\u20e3 Crear tabla (si no existe)\n    cursor.execute(\"\"\"\n        CREATE OR REPLACE TABLE RAW.TAXI_ZONE_LOOKUP (\n            LOCATIONID INT,\n            BOROUGH STRING,\n            ZONE STRING,\n            SERVICE_ZONE STRING\n        );\n    \"\"\")\n    print(\"\ud83d\udce6 Tabla RAW.TAXI_ZONE_LOOKUP lista para carga.\")\n\n    # 5\ufe0f\u20e3 Insertar datos por lotes\n    insert_sql = \"\"\"\n        INSERT INTO RAW.TAXI_ZONE_LOOKUP (LOCATIONID, BOROUGH, ZONE, SERVICE_ZONE)\n        VALUES (%s, %s, %s, %s)\n    \"\"\"\n\n    batch_size = 100\n    records = [tuple(x) for x in df.to_numpy().tolist()]\n    total = len(records)\n\n    for i in range(0, total, batch_size):\n        batch = records[i:i + batch_size]\n        cursor.executemany(insert_sql, batch)\n        conn.commit()\n        print(f\"\u2705 Insertado lote {i // batch_size + 1} ({len(batch)} filas)\")\n\n    cursor.close()\n    conn.close()\n\n    print(f\"\ud83c\udf89 Carga completada: {total} filas insertadas en RAW.TAXI_ZONE_LOOKUP\")\n", "file_path": "data_exporters/export_taxi_zone.py", "language": "python", "type": "data_exporter", "uuid": "export_taxi_zone"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/outstanding_resonance.py:data_exporter:python:outstanding resonance": {"content": "from sqlalchemy import create_engine, text\n\nengine = create_engine(\n    f\"snowflake://{user}:{password}@alnwmmv-nj56428.sa-east-1.aws/{database}/{schema}?warehouse={warehouse}&role={role}\"\n)\n\nwith engine.connect() as conn:\n    result = conn.execute(text(\"SELECT CURRENT_VERSION()\")).fetchone()\n    print(\"\u2705 Conectado a Snowflake. Versi\u00f3n:\", result[0])\n", "file_path": "data_exporters/outstanding_resonance.py", "language": "python", "type": "data_exporter", "uuid": "outstanding_resonance"}, "data_loaders/ingest_raw.py:data_loader:python:ingest raw": {"content": "import pandas as pd\nimport uuid\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Carga datos de Yellow y Green taxis para el a\u00f1o y mes indicados.\n    Devuelve un \u00fanico DataFrame concatenado con columna __service_type.\n    \"\"\"\n    year = kwargs.get(\"year\", 2024)\n    month = kwargs.get(\"month\", 4)\n\n    # Servicios a cargar\n    services = [\"yellow\", \"green\"]\n\n    all_dfs = []\n\n    for service in services:\n        file_name = f\"{service}_tripdata_{year}-{month:02d}.parquet\"\n        url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{file_name}\"\n        print(f\"\u2b07Cargando datos {service.upper()} de {year}-{month:02d} desde {url}\")\n\n        try:\n            df = pd.read_parquet(url)\n        except Exception as e:\n            print(f\"No se pudo cargar {service.upper()} {year}-{month:02d}: {e}\")\n            continue\n\n        # Metadatos\n        run_id = str(uuid.uuid4())\n        df.attrs[\"run_id\"] = run_id\n        df.attrs[\"year\"] = year\n        df.attrs[\"month\"] = month\n        df.attrs[\"service_type\"] = service\n        df.attrs[\"row_count\"] = len(df)\n\n        df[\"__run_id\"] = run_id\n        df[\"__year\"] = year\n        df[\"__month\"] = month\n        df[\"__service_type\"] = service\n\n        all_dfs.append(df)\n\n    if not all_dfs:\n        raise ValueError(\"No se pudo cargar ning\u00fan dataset (verifica URLs o meses).\")\n\n    # Unir todos los servicios\n    combined_df = pd.concat(all_dfs, ignore_index=True)\n\n    print(f\"Cargados {len(combined_df):,} registros totales \"\n          f\"de {len(all_dfs)} servicios ({', '.join(services)})\")\n\n    return combined_df\n\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, \"El output es None\"\n    assert len(output) > 0, \"El DataFrame est\u00e1 vac\u00edo\"\n    assert \"__service_type\" in output.columns, \"Falta columna __service_type\"\n", "file_path": "data_loaders/ingest_raw.py", "language": "python", "type": "data_loader", "uuid": "ingest_raw"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/gold/metadata.yaml:pipeline:yaml:gold/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: blue\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: dbt_env_setup_gold\n  retry_config: null\n  status: executed\n  timeout: null\n  type: custom\n  upstream_blocks: []\n  uuid: dbt_env_setup_gold\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/gold/dim_date.sql\n    file_source:\n      path: dbt/models/gold/dim_date.sql\n      project_path: dbt\n    limit: 1000\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/gold/dim_date\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/models/gold/dim_date\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/gold/dim_zone.sql\n    file_source:\n      path: dbt/models/gold/dim_zone.sql\n      project_path: dbt\n    limit: 1000\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/gold/dim_zone\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/models/gold/dim_zone\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/gold/dim_vendor.sql\n    file_source:\n      path: dbt/models/gold/dim_vendor.sql\n      project_path: dbt\n    limit: 1000\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/gold/dim_vendor\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/models/gold/dim_vendor\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/gold/dim_rate_code.sql\n    file_source:\n      path: dbt/models/gold/dim_rate_code.sql\n      project_path: dbt\n    limit: 1000\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/gold/dim_rate_code\n  retry_config: null\n  status: not_executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/models/gold/dim_rate_code\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/gold/dim_payment_type.sql\n    file_source:\n      path: dbt/models/gold/dim_payment_type.sql\n      project_path: dbt\n    limit: 1000\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/gold/dim_payment_type\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/models/gold/dim_payment_type\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/gold/dim_service_type.sql\n    file_source:\n      path: dbt/models/gold/dim_service_type.sql\n      project_path: dbt\n    limit: 1000\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/gold/dim_service_type\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/models/gold/dim_service_type\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/gold/dim_trip_type.sql\n    file_source:\n      path: dbt/models/gold/dim_trip_type.sql\n      project_path: dbt\n    limit: 1000\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/gold/dim_trip_type\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/models/gold/dim_trip_type\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/gold/fact_trips.sql\n    file_source:\n      path: dbt/models/gold/fact_trips.sql\n      project_path: dbt\n    limit: 1000\n  downstream_blocks:\n  - gold_test\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/gold/fact_trips\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/models/gold/fact_trips\n- all_upstream_blocks_executed: true\n  color: pink\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: gold_test\n  retry_config: null\n  status: executed\n  timeout: null\n  type: custom\n  upstream_blocks:\n  - dbt/models/gold/fact_trips\n  uuid: gold_test\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-10-05 18:35:55.005133+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: gold\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: gold\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/gold/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "gold/metadata"}, "pipelines/gold/__init__.py:pipeline:python:gold/  init  ": {"content": "", "file_path": "pipelines/gold/__init__.py", "language": "python", "type": "pipeline", "uuid": "gold/__init__"}, "pipelines/raw/metadata.yaml:pipeline:yaml:raw/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/ingest_raw.py\n  downstream_blocks:\n  - export_raw\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_raw\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_raw\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_raw\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - ingest_raw\n  uuid: export_raw\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-10-03 21:24:54.532746+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: raw\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: raw\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/raw/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "raw/metadata"}, "pipelines/raw/__init__.py:pipeline:python:raw/  init  ": {"content": "", "file_path": "pipelines/raw/__init__.py", "language": "python", "type": "pipeline", "uuid": "raw/__init__"}, "pipelines/silver_/metadata.yaml:pipeline:yaml:silver /metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: blue\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: dbt_env_setup\n  retry_config: null\n  status: executed\n  timeout: null\n  type: custom\n  upstream_blocks: []\n  uuid: dbt_env_setup\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt: {}\n    dbt_profile_target: null\n    dbt_project_name: dbt\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_path: dbt/models/silver/trips_clean.sql\n    file_source:\n      path: dbt/models/silver/trips_clean.sql\n      project_path: dbt\n    limit: 1000\n    use_raw_sql: false\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/silver/trips_clean\n  retry_config: null\n  status: failed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/models/silver/trips_clean\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/silver/trips_clean_green.sql\n    file_source:\n      path: dbt/models/silver/trips_clean_green.sql\n      project_path: dbt\n    limit: 1000\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/silver/trips_clean_green\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/models/silver/trips_clean_green\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/silver/trips_clean_all.sql\n    file_source:\n      path: dbt/models/silver/trips_clean_all.sql\n      project_path: dbt\n    limit: 1000\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/silver/trips_clean_all\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/models/silver/trips_clean_all\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt\n    file_path: dbt/models/silver/trips_audit.sql\n    file_source:\n      path: dbt/models/silver/trips_audit.sql\n      project_path: dbt\n    limit: 1000\n  downstream_blocks:\n  - silver_tests\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/models/silver/trips_audit\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/models/silver/trips_audit\n- all_upstream_blocks_executed: true\n  color: pink\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: silver_tests\n  retry_config: null\n  status: executed\n  timeout: null\n  type: custom\n  upstream_blocks:\n  - dbt/models/silver/trips_audit\n  uuid: silver_tests\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-10-04 16:37:51.801838+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: Silver'\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: silver_\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/silver_/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "silver_/metadata"}, "pipelines/silver_/__init__.py:pipeline:python:silver /  init  ": {"content": "", "file_path": "pipelines/silver_/__init__.py", "language": "python", "type": "pipeline", "uuid": "silver_/__init__"}, "pipelines/taxi_zones/metadata.yaml:pipeline:yaml:taxi zones/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_taxi_zone\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks: []\n  uuid: export_taxi_zone\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-10-05 00:07:31.424587+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: Taxi-zones\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: taxi_zones\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/taxi_zones/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "taxi_zones/metadata"}, "pipelines/taxi_zones/__init__.py:pipeline:python:taxi zones/  init  ": {"content": "", "file_path": "pipelines/taxi_zones/__init__.py", "language": "python", "type": "pipeline", "uuid": "taxi_zones/__init__"}, "/home/src/scheduler/dbt/models/silver/trips_clean.sql:dbt:sql:home/src/scheduler/dbt/models/silver/trips clean": {"content": "{{ config(\n    materialized = 'table',\n    alias = 'TRIPS_CLEAN'\n) }}\n\n\nWITH base AS (\n    SELECT\n        VendorID,\n        CAST(tpep_pickup_datetime AS TIMESTAMP_NTZ) AS pickup_ts,\n        CAST(tpep_dropoff_datetime AS TIMESTAMP_NTZ) AS dropoff_ts,\n        passenger_count::INT AS passenger_count,\n        trip_distance::FLOAT AS trip_distance,\n        fare_amount::FLOAT AS fare_amount,\n        tip_amount::FLOAT AS tip_amount,\n        total_amount::FLOAT AS total_amount,\n        payment_type::INT AS payment_type_raw,\n        PULocationID,\n        DOLocationID,\n        'yellow' AS service_type\n    FROM {{ source('RAW', 'YELLOW_TRIPS') }}\n),\n\nfiltered AS (\n    SELECT *\n    FROM base\n    WHERE\n        fare_amount > 0\n        AND trip_distance > 0\n        AND total_amount > 0\n        AND passenger_count BETWEEN 1 AND 8\n        AND pickup_ts IS NOT NULL\n        AND dropoff_ts IS NOT NULL\n        AND dropoff_ts >= pickup_ts\n        AND DATEDIFF('hour', pickup_ts, dropoff_ts) <= 12\n),\n\nenriched AS (\n    SELECT\n        f.*,\n        pu.ZONE AS pickup_zone,\n        pu.BOROUGH AS pickup_borough,\n        do.ZONE AS dropoff_zone,\n        do.BOROUGH AS dropoff_borough\n    FROM filtered f\n    LEFT JOIN {{ source('RAW', 'TAXI_ZONE_LOOKUP') }} pu\n        ON f.PULocationID = pu.LOCATIONID\n    LEFT JOIN {{ source('RAW', 'TAXI_ZONE_LOOKUP') }} do\n        ON f.DOLocationID = do.LOCATIONID\n)\n\nSELECT\n    VendorID,\n    pickup_ts,\n    dropoff_ts,\n    DATEDIFF('minute', pickup_ts, dropoff_ts) AS trip_minutes,\n    passenger_count,\n    trip_distance,\n    fare_amount,\n    tip_amount,\n    total_amount,\n    ROUND((tip_amount / NULLIF(fare_amount, 0)) * 100, 2) AS tip_percent,\n    CASE payment_type_raw\n        WHEN 1 THEN 'Credit card'\n        WHEN 2 THEN 'Cash'\n        WHEN 3 THEN 'No charge'\n        WHEN 4 THEN 'Dispute'\n        WHEN 5 THEN 'Unknown'\n        WHEN 6 THEN 'Voided trip'\n        ELSE 'Other'\n    END AS payment_type,\n    service_type,\n    pickup_zone,\n    pickup_borough,\n    dropoff_zone,\n    dropoff_borough,\n    YEAR(pickup_ts) AS pickup_year,\n    MONTH(pickup_ts) AS pickup_month,\n    DAY(pickup_ts) AS pickup_day,\n    DATE_TRUNC('month', pickup_ts) AS pickup_month_start\n\nFROM enriched\n", "file_path": "/home/src/scheduler/dbt/models/silver/trips_clean.sql", "language": "sql", "type": "dbt", "uuid": "dbt/models/silver/trips_clean"}, "/home/src/scheduler/dbt/models/silver/trips_clean_all.sql:dbt:sql:home/src/scheduler/dbt/models/silver/trips clean all": {"content": "{{ config(\n    materialized = 'table',\n    alias = 'TRIPS_CLEAN_ALL'\n) }}\n\nSELECT\n    VendorID,\n    pickup_ts,\n    dropoff_ts,\n    trip_minutes,\n    passenger_count,\n    trip_distance,\n    fare_amount,\n    tip_amount,\n    total_amount,\n    tip_percent,\n    payment_type,\n    service_type,\n    pickup_zone,\n    pickup_borough,\n    dropoff_zone,\n    dropoff_borough,\n    pickup_year,\n    pickup_month,\n    pickup_day,\n    pickup_month_start\nFROM {{ ref('trips_clean') }}\n\nUNION ALL\n\nSELECT\n    VendorID,\n    pickup_ts,\n    dropoff_ts,\n    trip_minutes,\n    passenger_count,\n    trip_distance,\n    fare_amount,\n    tip_amount,\n    total_amount,\n    tip_percent,\n    payment_type,\n    service_type,\n    pickup_zone,\n    pickup_borough,\n    dropoff_zone,\n    dropoff_borough,\n    pickup_year,\n    pickup_month,\n    pickup_day,\n    pickup_month_start\nFROM {{ ref('trips_clean_green') }}\n", "file_path": "/home/src/scheduler/dbt/models/silver/trips_clean_all.sql", "language": "sql", "type": "dbt", "uuid": "dbt/models/silver/trips_clean_all"}, "/home/src/scheduler/dbt/models/silver/trips_audit.sql:dbt:sql:home/src/scheduler/dbt/models/silver/trips audit": {"content": "{{ config(\n    materialized = 'table',\n    alias = 'TRIPS_AUDIT'\n) }}\n\n\n\nWITH raw_counts AS (\n    SELECT\n        'yellow' AS service_type,\n        COUNT(*) AS total_raw\n    FROM {{ source('RAW', 'YELLOW_TRIPS') }}\n    UNION ALL\n    SELECT\n        'green' AS service_type,\n        COUNT(*) AS total_raw\n    FROM {{ source('RAW', 'GREEN_TRIPS') }}\n),\n\nclean_counts AS (\n    SELECT\n        service_type,\n        COUNT(*) AS total_clean\n    FROM {{ ref('trips_clean_all') }}\n    GROUP BY service_type\n)\n\nSELECT\n    CURRENT_TIMESTAMP() AS audit_ts,\n    r.service_type,\n    r.total_raw,\n    c.total_clean,\n    r.total_raw - c.total_clean AS records_removed,\n    ROUND((c.total_clean / NULLIF(r.total_raw, 0)) * 100, 2) AS pct_retained\n\n\nFROM raw_counts r\nLEFT JOIN clean_counts c\n    ON r.service_type = c.service_type\n", "file_path": "/home/src/scheduler/dbt/models/silver/trips_audit.sql", "language": "sql", "type": "dbt", "uuid": "dbt/models/silver/trips_audit"}, "/home/src/scheduler/custom/dbt_env_setup.py:custom:python:home/src/scheduler/custom/dbt env setup": {"content": "import os\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef dbt_env_setup(*args, **kwargs):\n    env_vars = {\n        \"SNOWFLAKE_USER\": get_secret_value(\"SNOWFLAKE_USER\"),\n        \"SNOWFLAKE_PASSWORD\": get_secret_value(\"SNOWFLAKE_PASSWORD\"),\n        \"SNOWFLAKE_ACCOUNT\": get_secret_value(\"SNOWFLAKE_ACCOUNT\"),\n        \"SNOWFLAKE_DEFAULT_WH\": get_secret_value(\"SNOWFLAKE_DEFAULT_WH\"),\n        \"SNOWFLAKE_DEFAULT_DB\": get_secret_value(\"SNOWFLAKE_DEFAULT_DB\"),\n        \"SNOWFLAKE_DEFAULT_SCHEMA\": get_secret_value(\"SNOWFLAKE_DEFAULT_SCHEMA\"),\n        \"SNOWFLAKE_ROLE\": get_secret_value(\"SNOWFLAKE_ROLE\"),\n    }\n\n    for key, value in env_vars.items():\n        if value:\n            os.environ[key] = str(value)\n\n    os.environ[\"DBT_PROFILES_DIR\"] = \"/home/src/scheduler/dbt\"\n    os.environ[\"DBT_PROFILE_NAME\"] = \"interpolated\"\n    os.environ[\"DBT_DEFAULT_SCHEMA\"] = \"SILVER\"\n\n    print(\"Variables de entorno configuradas para DBT (capa SILVER):\")\n    print(f\"DBT_PROFILES_DIR = {os.environ['DBT_PROFILES_DIR']}\")\n    print(f\"DBT_PROFILE_NAME = {os.environ['DBT_PROFILE_NAME']}\")\n    print(f\"DBT_DEFAULT_SCHEMA = {os.environ['DBT_DEFAULT_SCHEMA']}\")\n    print(\"Variables cargadas:\", \", \".join(env_vars.keys()))\n\n\n@test\ndef test_output(*args, **kwargs):\n    required_vars = [\n        \"SNOWFLAKE_USER\", \"SNOWFLAKE_PASSWORD\", \"SNOWFLAKE_ACCOUNT\",\n        \"SNOWFLAKE_DEFAULT_WH\", \"SNOWFLAKE_DEFAULT_DB\",\n        \"SNOWFLAKE_DEFAULT_SCHEMA\", \"SNOWFLAKE_ROLE\",\n        \"DBT_PROFILES_DIR\", \"DBT_PROFILE_NAME\", \"DBT_DEFAULT_SCHEMA\"\n    ]\n    missing = [v for v in required_vars if v not in os.environ]\n    assert not missing, f\"Faltan variables: {missing}\"\n    print(\"Variables correctamente configuradas.\")\n", "file_path": "/home/src/scheduler/custom/dbt_env_setup.py", "language": "python", "type": "custom", "uuid": "dbt_env_setup"}, "/home/src/scheduler/custom/silver_tests.py:custom:python:home/src/scheduler/custom/silver tests": {"content": "import subprocess\nimport os\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n@custom\ndef run_dbt_tests(*args, **kwargs):\n    project_dir = \"/home/src/scheduler/dbt\"\n    command = [\n        \"dbt\", \"test\",\n        \"--project-dir\", project_dir,\n        \"--profiles-dir\", project_dir,\n        \"--select\", \"silver.*\"\n    ]\n\n    print(f\" Ejecutando DBT tests en {project_dir}...\")\n    result = subprocess.run(command, capture_output=True, text=True)\n\n    print(\" STDOUT:\\n\", result.stdout)\n    print(\" STDERR:\\n\", result.stderr)\n\n    if result.returncode == 0:\n        print(\" Tests completados correctamente.\")\n    else:\n        raise Exception(\" Error en los tests DBT.\")\n", "file_path": "/home/src/scheduler/custom/silver_tests.py", "language": "python", "type": "custom", "uuid": "silver_tests"}, "/home/src/scheduler/dbt/models/gold/dim_trip_type.sql:dbt:sql:home/src/scheduler/dbt/models/gold/dim trip type": {"content": "{{ config(\n    materialized = 'table',\n    alias = 'DIM_TRIP_TYPE'\n) }}\n\n{% call statement('create_gold_schema', fetch_result=False) %}\n    CREATE SCHEMA IF NOT EXISTS {{ target.database }}.GOLD;\n{% endcall %}\n\n\nSELECT * FROM (\n    SELECT 1 AS trip_type, 'Street-hail' AS trip_type_desc UNION ALL\n    SELECT 2 AS trip_type, 'Dispatch' AS trip_type_desc UNION ALL\n    SELECT 99 AS trip_type, 'Unknown' AS trip_type_desc\n)\n", "file_path": "/home/src/scheduler/dbt/models/gold/dim_trip_type.sql", "language": "sql", "type": "dbt", "uuid": "dbt/models/gold/dim_trip_type"}, "/home/src/scheduler/dbt/models/gold/fact_trips.sql:dbt:sql:home/src/scheduler/dbt/models/gold/fact trips": {"content": "{{ config(\n    materialized = 'table',\n    alias = 'FACT_TRIPS'\n) }}\n\n\n\n{% call statement('create_gold_schema', fetch_result=False) %}\n    CREATE SCHEMA IF NOT EXISTS {{ target.database }}.GOLD;\n{% endcall %}\n\n\nWITH base AS (\n    SELECT\n        pickup_ts,\n        dropoff_ts,\n        DATEDIFF('minute', pickup_ts, dropoff_ts) AS trip_minutes,\n        trip_distance,\n        fare_amount,\n        tip_amount,\n        total_amount,\n        passenger_count,\n        service_type,\n        pickup_zone,\n        pickup_borough,\n        dropoff_zone,\n        dropoff_borough,\n        payment_type,\n        VendorID,\n        pickup_year,\n        pickup_month,\n        pickup_day\n    FROM {{ source('SILVER', 'TRIPS_CLEAN_ALL') }}\n),\n\n\njoined AS (\n    SELECT\n        -- \ud83d\udd52 Fecha\n        d.date_actual AS trip_date,\n        d.year AS trip_year,\n        d.month AS trip_month,\n        d.day AS trip_day,\n\n        -- \ud83c\udf0d Zonas\n        z.zone_name AS pickup_zone,\n        z.borough_name AS pickup_borough,\n\n        -- \ud83d\ude96 Dimensiones clave\n        v.vendor_id,\n        p.payment_type_id,\n        s.service_type_id,\n        r.rate_code_id,\n        t.trip_type,\n\n        -- \ud83e\uddee M\u00e9tricas\n        b.trip_distance,\n        b.trip_minutes,\n        b.fare_amount,\n        b.tip_amount,\n        b.total_amount,\n        ROUND((b.tip_amount / NULLIF(b.fare_amount, 0)) * 100, 2) AS tip_percent,\n        b.passenger_count\n\n    FROM base b\n\n    LEFT JOIN {{ ref('dim_date') }} d\n        ON b.pickup_year = d.year\n       AND b.pickup_month = d.month\n       AND b.pickup_day = d.day\n\n    LEFT JOIN {{ ref('dim_zone') }} z\n        ON b.pickup_zone = z.zone_name\n\n    LEFT JOIN {{ ref('dim_vendor') }} v\n        ON b.VendorID = v.vendor_id\n\n    LEFT JOIN {{ ref('dim_payment_type') }} p\n        ON b.payment_type = p.payment_type_desc\n\n    LEFT JOIN {{ ref('dim_service_type') }} s\n        ON b.service_type = s.service_desc\n\n    LEFT JOIN {{ ref('dim_rate_code') }} r\n        ON 1 = 1  -- Sin campo directo, todos los viajes usan la referencia base\n\n    LEFT JOIN {{ ref('dim_trip_type') }} t\n        ON 1 = 1  -- Sin campo directo, referencia general\n),\n\n\naggregated AS (\n    SELECT\n        trip_year,\n        trip_month,\n        pickup_borough,\n        pickup_zone,\n        COUNT(*) AS total_trips,\n        ROUND(AVG(trip_distance), 2) AS avg_distance_km,\n        ROUND(AVG(trip_minutes), 2) AS avg_trip_minutes,\n        ROUND(AVG(fare_amount), 2) AS avg_fare_amount,\n        ROUND(AVG(tip_amount), 2) AS avg_tip_amount,\n        ROUND(AVG(tip_percent), 2) AS avg_tip_percent,\n        ROUND(SUM(total_amount), 2) AS total_revenue\n    FROM joined\n    GROUP BY 1, 2, 3, 4\n)\n\nSELECT * FROM aggregated\n", "file_path": "/home/src/scheduler/dbt/models/gold/fact_trips.sql", "language": "sql", "type": "dbt", "uuid": "dbt/models/gold/fact_trips"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}